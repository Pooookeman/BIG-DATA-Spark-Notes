# Week 3. MapReduce and Hadoop (Continued)

## Dealing with failures
1. master node fail
    - Restart entire MapReduce job
2. compute node of a map worker fail
    - Reset completed or in-progress map tasks at worker to idle
    - Restart **all** the map tasks assigned to this node -> lose intermediate files from this node
    - Inform Reduce workers when task is rescheduled on another worker, location of input from that map task has changed
3. compute node of a reduce worker fail
    - **Only** reset in-progress tasks to idle
    - Restart these reduce tasks at another node

## Number of Map and Reduce jobs to allocate
- M map tasks, R reduce tasks
- make M > number of nodes in the cluster
    - One DFS chunk per map is common
    - Improves dynamic load balancing and speeds up recovery from worker failures
- Usually R < M
    - Because output is spread across R files

## Why Map outputs to Local Disk?
- Map output is intermediate
    - To be processed by reduce task to produce final output
    - Can be discarded after job is complete 
    - Storing in DFS with replication is overkill
- Automatically rerun map task on another node to recreate the map output if the node running map task fails before the map output is consumed by reduce task

## Refinement
1. Backup Tasks
 - Slow workers significantly lengthen job completion time
    - Other jobs on the machine 
    - Bad disks
    - Weird things
- Solution
    - Near end of phase, spawn _(refers to a function that loads and executes a new child process)_ backup copies of tasks
        - Whichever one finishes first “wins”
- Effect
    - Dramatically shortens job completion time
2. Combiners(可结合 w2_extension.md 理解)
- A Map task often produce many pairs of the form (k,v1), (k,v2), ... for the same key k
- A Combiner is a local aggregation function for repeated keys produced by the same map
    - For associative operations such as sum, count, max 
    - Decreases size of intermediate data
    - Save network use
    - Example local counting for Word Count def combiner (key, values): output (key, sum(values))
![](graphs/w3/combiners.png)
3. Partition FUnction 
- control how keys are partitioned (特殊需要，不用default)
- Reduce needs to ensure that records with the same intermediate key end up at the same worker
    - System uses a default partition function: hash(key) mod R 
    - Sometimes useful to override the hash function:
        - e.g., hash(hostname(URL)) mod R ensures URLs from a host end up in the same output file