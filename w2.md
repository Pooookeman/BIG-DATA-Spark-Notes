# Week 2. MapReduce and Hadoop

## Paralleism

1. Why?
    - increasing data size
    - limitation of single node architecture
    - emerging affordable cluster architecture 

2. How?
    - Cluster Architescture: "many" nodes

## Large Scale Computation
Tools: hadoop mapreduce; Hive; Spark; Spark Streaming; kafka

Challenge 1: nodes failure\
Soln 1:Build fault-tolerance into the storage infrastructure
- Distributed File System
- Replicate files multiple times

Challenge 2: low bandwidth\
Soln 2: Bring computation close to the data

Challenge 3: distribution system is hard to code\
soln 3: (MapReduce) data-parallel programming

## **Key components for Hadoop:**
- Distributed File System
- MapReduce Programming System
    - Computations are divided into multiple tasks -> If one task failed, it can be restarted without affecting other tasks.


## Distributed File System
### Properties:

1. Files: *BIG*
2. Typical Usage:
    - rarely updated in place
    - reads and append-only
3. Files are solit into contiguous chunks(62-128 MB)
    - each chunk is replicated (~3 times)
    - replicas kept in different racks
4. Master Node(Namenode)
    - Stores metadata about where files are located
    - may be replicated
5. Client library for file access
    - talks to master node to locate the chunk servers
    - connects directly to chunk servers to access data
6. Wrap-up:
    - data is kept in "chunks" across machines
    - each chunk is *replicated* on different machines
    - seamless recover from disk or machine failure
    - bring computation *directly* to the data
    - chunk servers also serve as compute servers

## MapReduce

1. What is it?\
Data-parallel programming model for clusters of commodity machines
    - *sequentially* read data
    - `Map`: Extract something you care about
    - group by key: Sort and Shuffle
    - `Reduce:` Aggregate, summarize, filter or transform
    - write the result
2. MapReduce Programming Model
    - data types: (key,value)
        - keys do not have to be unique
        - types of keys and values are arbitrary
    - Input: bag of (key, value) pair
    - `Map` function: (k<sub>in</sub>, v<sub>in</sub>) -> \<k,v><sup>*</sup>
        - take (key,value) pairs as input and output a *set* of (key,value) pairs
        - One Map call for each (k<sub>in</sub>, v<sub>in</sub>) pair
    - `Reduce` function: (k, \<v><sup>*</sup>) -> (k<sub>out</sub>, v<sub>out</sub>)<sup>\*</sup>
        - all pairs with the same key are reduced together
        - One Reduce call for per unique key
    - Reduce function cannot start until all map functions are done
    - Group by key
        - user define the number of Reduce tasks R
        - system hash the function to apply to keys, produce a bucket number from 0 to R-1
        - hash each key output by Map task, put key-value pair in one of the R local files
        - Master controller merge files from Map tasks destined for the same Reduce task as a sequence of (key, list of values) pairs
3. MapReduce Data Flow
     - Input and final output are stored on a distributed file system(FS)
     - Intermediate results are stored on local FS
        - Allows recovery if a reducer crashes
     - Output can be input to another MapReduce task
4. MapReduce Coordination
    - Master node takes care o coordination
         - task status: idle, in-progess, completed
         - idle tasks get scheduled as workers become available
    - When a map task completes, it sends the master the location and sizes of its R intermediate files, one for each reducer
    - Master pushes this information to reducers
    - Master pings workers periodically to detect failures
    - Output of Reduce task is stored in HDFS for reliability